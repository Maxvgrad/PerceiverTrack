#!/bin/bash -l

#SBATCH -A bolt
#SBATCH --job-name="bolt-internship-train-trackformer-drop-crowdhuman"
#SBATCH --time=24:00:00
#SBATCH --partition=gpu
#SBATCH --output=not_tracked_dir/slurm/%j_slurm_%x.out

# Resource allocation
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --mem=64G

# Node configurations (commented out)
## falcone configurations
#SBATCH --gres=gpu:tesla:4
#SBATCH --cpus-per-task=4

## Pegasus configuration
##SBATCH --gres=gpu:a100-40g:4
##SBATCH --cpus-per-task=24

## Pegasus2 configuration
##SBATCH --gres=gpu:a100-80g:2
##SBATCH --cpus-per-task=16

#----------------------------------------
# Parse Arguments
#----------------------------------------
wandb_id=None
resume_optim=False
resume=""  # No resume
timestamp=$(date +%Y-%m-%d_%H-%M-%S)
output_dir="not_tracked_dir/output_${model_name}_${timestamp}"

while getopts "r:wid:ro:o" opt; do
  case ${opt} in
    o )
      output_dir=$OPTARG
      ;;
    r )
      resume=$OPTARG
      ;;
    ro )
      resume_optim=True
      ;;
    wid )
      wandb_id=$OPTARG
      ;;
    \? )
      echo "Usage: cmd -o output dir [-r] checkpoint [-ro] to enable optim [-wid] wandb id to continue logging"
      exit 1
      ;;
  esac
done

#----------------------------------------
# Environment Setup
#----------------------------------------
module load miniconda3
conda activate perceiver_track

#----------------------------------------
# Distributed Training Setup
#----------------------------------------
# Set master node address
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
export MASTER_PORT=12381

# Calculate world size for distributed training
export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))

# Enable NCCL debugging
export NCCL_DEBUG=INFO

#----------------------------------------
# Debug Information
#----------------------------------------
echo "MASTER_ADDR=$MASTER_ADDR"
echo "WORLD_SIZE=$WORLD_SIZE"
echo "SLURM_PROCID=$SLURM_PROCID"
echo "RANK=$RANK"
echo "OMP_NUM_THREADS=$OMP_NUM_THREADS"

echo "output_dir=$output_dir"
echo "resume=$resume"
echo "resume_optim=$resume_optim"
echo "wandb_id=$wandb_id"


# Launch training
srun python src/train.py with \
    crowdhuman \
    deformable \
    multi_frame \
    tracking \
    world_size=$WORLD_SIZE \
    output_dir=$output_dir \
    wandb_project='trackformer-drop-train' \
    track_query_false_negative_prob=0 \
    track_query_false_positive_prob=0 \
    frame_dropout_prob=0.3 \
    tracking_eval=False \
    wandb_id="$wandb_id" \
    resume_optim="$resume_optim" \
    resume="$resume"