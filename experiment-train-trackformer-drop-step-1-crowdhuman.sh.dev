#!/bin/bash -l

#SBATCH -A bolt
#SBATCH --job-name="bolt-internship-train-trackformer-drop-crowdhuman"
#SBATCH --time=24:00:00
#SBATCH --partition=gpu
#SBATCH --output=not_tracked_dir/slurm/%j_slurm_%x.out

# Resource allocation
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --mem=64G

# Node configurations (commented out)
## falcone configurations
#SBATCH --gres=gpu:tesla:4
#SBATCH --cpus-per-task=4

## Pegasus configuration
##SBATCH --gres=gpu:a100-40g:4
##SBATCH --cpus-per-task=24

## Pegasus2 configuration
##SBATCH --gres=gpu:a100-80g:2
##SBATCH --cpus-per-task=16

#----------------------------------------
# Parse Arguments
#----------------------------------------
resume_flag=false
wandb_id=None
while getopts "r:w:" opt; do
  case ${opt} in
    r )
      resume_flag=true
      ;;
    w )
      wandb_id=$OPTARG
      ;;
    \? )
      echo "Usage: cmd [-r] to enable resume [-w] wandb id to continue logging"
      exit 1
      ;;
  esac
done

#----------------------------------------
# Environment Setup
#----------------------------------------
module load miniconda3
conda activate perceiver_track

#----------------------------------------
# Directory Setup
#----------------------------------------
output_dir="models/crowdhuman_trackformer_drop"

#----------------------------------------
# Distributed Training Setup
#----------------------------------------
# Set master node address
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
export MASTER_PORT=12381

# Calculate world size for distributed training
export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))

# Enable NCCL debugging
export NCCL_DEBUG=INFO

#----------------------------------------
# Debug Information
#----------------------------------------
echo "MASTER_ADDR=$MASTER_ADDR"
echo "WORLD_SIZE=$WORLD_SIZE"
echo "SLURM_PROCID=$SLURM_PROCID"
echo "RANK=$RANK"
echo "output_dir=$output_dir"
echo "OMP_NUM_THREADS=$OMP_NUM_THREADS"

#----------------------------------------
# Training Command
#----------------------------------------

# Set parameters based on resume flag
if [ "$resume_flag" = true ]; then
    checkpoint=$output_dir/checkpoint.pth
    resume_optim=true
    resume=$checkpoint
    echo "Resuming training from checkpoint: $checkpoint"
else
    wandb_id=None  # Set this to an empty string or a default value if necessary
    resume_optim=false
    resume=""  # No resume
    echo "Starting new training run."
fi

# Launch training
srun python src/train.py with \
    crowdhuman \
    deformable \
    multi_frame \
    tracking \
    world_size=$WORLD_SIZE \
    output_dir=$output_dir \
    wandb_project='trackformer-drop-train' \
    track_query_false_negative_prob=0 \
    track_query_false_positive_prob=0 \
    frame_dropout_prob=0.3 \
    tracking_eval=False \
    wandb_id="$wandb_id" \
    resume_optim="$resume_optim" \
    resume="$resume"