#!/bin/bash -l

#SBATCH -A bolt
#SBATCH --job-name="bolt-internship-train-trackformer-private-step-1-crowdhuman"
#SBATCH --time=24:00:00
#SBATCH --partition=gpu
#SBATCH --output=not_tracked_dir/slurm/%j_slurm_%x.out

# Resource allocation
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --mem=16G

# Node configurations (commented out)
## falcone configurations
#SBATCH --gres=gpu:tesla:1
#SBATCH --cpus-per-task=4

## Pegasus configuration
##SBATCH --gres=gpu:a100-40g:1
##SBATCH --cpus-per-task=24

## Pegasus2 configuration
##SBATCH --gres=gpu:a100-80g:1
##SBATCH --cpus-per-task=16

#----------------------------------------
# Parse Arguments
#----------------------------------------
checkpoint_file=""
output_dir=""
while getopts "rc:" opt; do
  case ${opt} in
    o )
      output_dir=$OPTARG
      ;;
    c )
      checkpoint_file=$OPTARG
      ;;
    \? )
      echo "Usage: cmd [-r] to enable resume, -o <output_dir> -c <checkpoint_file> to specify a checkpoint file"
      exit 1
      ;;
  esac
done

if [ -z "$checkpoint_file" ]; then
    echo "Error: -c <checkpoint_file> is required."
    exit 1
fi

if [ -z "output_dir" ]; then
    echo "Error: -o <output_dir> is required."
    exit 1
fi

#----------------------------------------
# Environment Setup
#----------------------------------------
module load miniconda3
conda activate perceiver_track

#----------------------------------------
# Distributed Training Setup
#----------------------------------------
# Set master node address
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr
export MASTER_PORT=12381

# Calculate world size for distributed training
export WORLD_SIZE=$(($SLURM_NNODES * $SLURM_NTASKS_PER_NODE))

# Enable NCCL debugging
export NCCL_DEBUG=INFO

#----------------------------------------
# Debug Information
#----------------------------------------
echo "MASTER_ADDR=$MASTER_ADDR"
echo "WORLD_SIZE=$WORLD_SIZE"
echo "SLURM_PROCID=$SLURM_PROCID"
echo "RANK=$RANK"
echo "output_dir=$output_dir"
echo "checkpoint_file=$checkpoint_file"
echo "OMP_NUM_THREADS=$OMP_NUM_THREADS"

seeds=(301)

# Loop over the seeds and run the training script
for seed in "${seeds[@]}"; do
    echo "Running evaluation with seed $seed"
    python src/train.py with \
        mot17 \
        deformable \
        batch_size=1 \
        multi_frame \
        world_size=$WORLD_SIZE \
        output_dir=$output_dir \
        wandb_project='trackformer-train-from-scratch' \
        resume="${output_dir}/${checkpoint_file}" \
        eval_only=True \
        tracking_eval=False \
        tracking=True \
        seed=$seed \
        disable_propagate_track_query_experiment=True \
        sequence_frames=16
done